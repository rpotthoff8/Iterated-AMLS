clear all

%Script calls the wahba functions to test noisy function approximation with self terminating criteria.
%User options at the top can be set to run repeat tests over a range
%termination percentages.
%to compare computation time and accuracy.  Or User options can be
%adjusted to perform a single test.  User options can also be selected to randomly
%split the data into a desired percentage of training/testing data or use a
%presplit dataset.  If performing repeat tests, the data is split for every repeat test. Called functions do not need to be provided testing or
%true data and can run with only training data, although the default of
%this script is to provide both.  User option 'train_percent' can be set to
%1 to provide only training data.

%USER OPTIONS SECTION
%   For performing single test: set 'numRepeats=1', 'options.plot=1'
%   (recommended), and single value for 'terminate_percentage',
%   'repeat_plots=0' (recommended)

%  If desired continue until 'numRBF' have been placed (not use noise self
%  terminating criteria): set 'data.noise_std=0'
%  If desired to use self-termination criteria: for best possible guess of
%  noise, set 'data.noise_std=noise_std'

%DATA INPUT OPTIONS
presplit_data=0; %1 for using presplit train/test data files, 0 for performing new random split for this test
train_percent=.7; %Percentage of data to be used for training: Only used if 'presplit_data==0'
data.noise_std_true=0.03; %Standard deviation for noise to be added to data as percent of true fuction max
%Enter number of desired test function. Options are:
% 1: 1-D wahba function
% 2: 1-D Polynomial
% 3: 1-D Sin function
% 4: 2-D initial 2D Wahba test function
% 5: 2-D Rosenbrock function
% 6: 2-D Goldstein-Price function
% 7: 2-D Cubic Function
test_function=1;

%TESTING OPTIONS
numRepeats=1; %Number of repeat runs to be performed (Mainly for getting accurate computation time)
numRBF=50; %Max number of RBFs to be placed
% full_methods={'wahba5_2D_DQ';'wahba5_2D_DQmultideriv';'wahba5_2D_DQmultideriv_vW';'wahba5_2D_DQmultideriv_bestW';'wahba5_2D_DQmultideriv_vW_minTest';'wahba5_2D_DQmultideriv_vW_minTest_terminate'}; %Full list of method function names
data.noise_std_guess=data.noise_std_true; %Guess of noise standard deviation for use in self termination: For best possible guess; data.noise_std=data.noise_std_true, For placing max numRBF; data.noise_std=0
terminate_percentages=[.92]; %Cutoff percentage of points for self termination: recommended; terminate_percentages=0.92
% h_values=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,0.98,0.99]; %h for determining RBF shape factor
 h_values=.9;

%OUTPUT OPTIONS
plot=1; %If plotter should be used within each RBF placer function: not recommended for repeat tests
repeat_plots=0; %User option if plots sould be generated following all tests showing mean results (generally only useful if repeats are being performed)
Final_approx_plot=1; %Plot final approximation, residuals, and RBF locations
save_figures=0; %User option to save repeat figures generated by tester
save_data=0; %User option to save data

% Method Options
methods=logical(1);
labels={"Iterated AMLS"};
opt_funct={"Null"};
self_terminate={0};
best_test={0};
drivers(:)={'Iterated_AMLS'};

%END USER OPTIONS SECTION
full_options=struct('opt_funct',opt_funct,'self_terminate',self_terminate,'label',labels,'plot',plot,'best_test',best_test);

%% START DATA LOADING SECTION
if presplit_data==1 %Load data file containing training, testing, and true data
    load('presplit_data.mat')
    if isfield(data,'x_test')
        test_data=1;
    else
        test_data=0;
    end
else %Perform random split of data into testing and training segments
    
    
    if test_function<=3
        Final_approx_plot=0;
    end
    
    if test_function==1
        data.x_full=[0:0.05:2]';
        a = 3.25;
        data.f_true = 4.26*(exp(-a*data.x_full) - 4*exp(-2*a*data.x_full) + 3*exp(-3*a*data.x_full));
        
    elseif test_function==2
        %polynomial function
        data.x_full=[-4:.1:4]';
        %Define the signal
        a =50.0;
        data.f_true = 1.0 + data.x_full.^2/a;

    elseif test_function==3
        %sin function
        data.x_full=[-pi:0.05:pi]';
        %Define the signal
        a=1/3;
        data.f_true =a* sin(data.x_full);

    else
        
        %Define the 2D true signal
        Grid = [0:0.04:1];
        [X,Y] = meshgrid(Grid,Grid); %uncomment for 2D
        %     [X,Y,Z] = meshgrid(Grid,[0:0.05:.9],Grid); %comment for 2D
        x=X(:);
        y=Y(:);
        %     z=Z(:); %comment for 2D
        data.x_full(:,1)=x;
        data.x_full(:,2)=y;
        %     data.x_full(:,3)=z; %comment for 2D
        for i=1:size(data.x_full,2)
            data.x_grid_dim(i)=size(X,i);
        end
        
        if test_function==4
            %Initial Test Function:
            data.f_true = (3/4)*exp(-(1/4)*((9*x-2).^2+(9*y-2).^2))+...
                (3/4)*exp(-(1/49)*(9*x+1).^2 - (1/10)*(9*y+1).^2)+...
                (1/2)*exp(-(1/4)*((9*x-7).^2+(9*y-3).^2))-...
                (1/5)*exp(-(9*x-4).^2-(9*y-7).^2);
        elseif test_function==5
            %Rosenbrock function:
            a=.5;
            b=100;
            data.f_true=(a-x).^2+b.*(y-x.^2).^2;
        elseif test_function==6
            % Goldstein-Price function:
            data.f_true=(1+((x+y+1).^2).*(19-14*x+3*x.^2-14*y+6.*x.*y+3*y.^2)).*(30+((2*x+3*y).^2).*(18+32*x+12*x.^2+48*y-36*x.*y+27*y.^2));
        elseif test_function==7
            data.f_true=x.^3+y.^3;
        
        end
    end

    noise=data.noise_std_true*(max(data.f_true)-min(data.f_true))*randn(size(data.f_true));
    f0_full=data.f_true+noise;
    actualstdev = std(data.f_true - f0_full)
    R_actual = (data.f_true) - f0_full;
    data.err_actual = sqrt(mean(R_actual.^2));
    
    %Split into training and testing data
    if train_percent<1
        test_data=1;
    else
        test_data=0;
        data.x=data.x_full;
        data.f0=f0_full;
    end
    
end
%END DATA SECTION

%% START TESTING SECTION
drivers_use=drivers(methods); %Methods to be tested
options_use=full_options(methods);

%Initialize variables
time=zeros(numRepeats,numel(h_values),size(drivers_use,1));
err_train=zeros(numRepeats,numel(h_values),size(drivers_use,1));
err_hist_train=zeros(numRBF+1, numRepeats,numel(h_values),size(drivers_use,1));
nlfit_train=zeros(3, numRepeats,numel(h_values),size(drivers_use,1));

if test_data==1
    err_test=zeros(numRepeats,numel(h_values),size(drivers_use,1));
    err_hist_test=zeros(numRBF+1, numRepeats,numel(h_values),size(drivers_use,1));
    nlfit_test=zeros(3, numRepeats,numel(h_values),size(drivers_use,1));
end
if isfield(data,'f_true')
    err_true=zeros(numRepeats,numel(h_values),size(drivers_use,1));
    err_hist_true=zeros(numRBF+1, numRepeats,numel(h_values),size(drivers_use,1));
    nlfit_true=zeros(3, numRepeats,numel(h_values),size(drivers_use,1));
end
used_RBF=zeros(numRepeats,numel(h_values),size(drivers_use,1));

for j=1:numRepeats %Outer loop performs repeat tests for obtaining accurate average computation time for different methods
    pause(1)
    %Randomly split data for each repeat
    if test_data==1 && presplit_data==0
        trainI=AOX_LHS(ones(size(data.x_full,1)),data.x_full,train_percent); %LHS sample for training point indices: always includes edges
        data.x=data.x_full(trainI,:);
        data.f0=f0_full(trainI,:);
        data.x_test=data.x_full;
        data.x_test(trainI,:)=[];
        data.f0_test=f0_full;
        data.f0_test(trainI,:)=[];
    end
    j
    for i=1:numel(h_values) %Loop controls range of cutoff percentages to be tested
        %         options.terminate_percent=terminate_percentages;
        
        
        for k=1:numel(drivers_use) %Loop through the methods to be tested
            options=options_use(k);
            options.fignum=k;
            options.h=h_values(i);
            
            tic %Start timing
            eval(['[xc_temp,eps_temp,mult_temp,h_temp,err_temp,err_hist_temp] = ' char(drivers_use(k)) '(numRBF,data,options);']) %Run method, Store error in temporary variable
            time(j,i,k)=toc; %End timing
            %Record number of RBFs used
            used_RBF(j,i,k)=size(xc_temp,2);
            %Store RBF properties
            xc{k}=xc_temp;
            eps{k}=eps_temp;
            mult{k}=mult_temp;
            h{k}=h_temp;
            
            %Split error into training, true, and testing variables
            err_train(j,i,k)=err_temp.train;
            err_hist_train(1:size(err_hist_temp.train,1),j,i,k)=err_hist_temp.train; %Index: 1st=iteration, 2nd=repeat number, 3rd=termination percentage, 4th=method
            %nfit equation:
            predictors=[ones(size(err_hist_temp.train,1)-1,1),log(2:size(err_hist_temp.train,1))',log(log(2:size(err_hist_temp.train,1)))'];
            nlfit_train(:,j,i,k)=regress(err_hist_temp.train(2:end),predictors);
            
            if test_data==1
                err_test(j,i,k)=err_temp.test;
                err_hist_test(1:size(err_hist_temp.test,1),j,i,k)=err_hist_temp.test; %Index: 1st=iteration, 2nd=repeat number, 3rd=termination percentage, 4th=method
                nlfit_test(:,j,i,k)=regress(err_hist_temp.test(2:end),predictors);
            end
            if isfield(err_temp,'true')
                err_true(j,i,k)=err_temp.true;
                err_hist_true(1:size(err_hist_temp.true,1),j,i,k)=err_hist_temp.true; %Index: 1st=iteration, 2nd=repeat number, 3rd=termination percentage, 4th=method
                nlfit_true(:,j,i,k)=regress(err_hist_temp.true(2:end),predictors);
            end
            
        end
    end
end

%CALCULATING MEANS FOR ALL RUNS
mean_time=mean(time,1);
mean_err_train=mean(err_train,1);
mean_err_hist_train=mean(err_hist_train,2);
if test_data==1
    mean_err_test=mean(err_test,1);
    mean_err_hist_test=mean(err_hist_test,2);
end
if isfield(err_temp,'true')
    mean_err_true=mean(err_true,1);
    mean_err_hist_true=mean(err_hist_true,2);
    true_error=1;
end
mean_used_RBF=mean(used_RBF,1);


%CALCULATING STD FOR ALL RUNS
time_std=std(time,0,1);
err_train_std=std(err_train,0,1);
err_hist_train_std=std(err_hist_train,0,2);
if test_data==1
    err_test_std=std(err_test,0,1);
    err_hist_test_std=std(err_hist_test,0,2);
end
if true_error==1
    err_true_std=std(err_true,0,1);
    err_hist_true_std=std(err_hist_true,0,2);
end
used_RBF_std=std(used_RBF,0,1);

runLabel=datestr(now,'yyyy-mmdd-HHMMSS');
%Save data
if save_data==1
    save(strcat('wahba2D_result_',runLabel))
end

%% Final Approximation plot
if Final_approx_plot==1
    figure(k+5)
    total_subplots=k+2;
    subplot(2,ceil(total_subplots/2),1)
    surf(X,Y,reshape(data.f_true,sqrt(numel(data.f_true)),sqrt(numel(data.f_true))))
    title('True Function');
    subplot(2,ceil(total_subplots/2),2)
    surf(X,Y,reshape(f0_full,sqrt(numel(f0_full)),sqrt(numel(f0_full))))
    title('Provided Noisy Function');
    sgtitle('Function Approximation')
    
    figure(k+6)
    Res_total_subplots=k+1;
    subplot(2,ceil(Res_total_subplots/2),1)
    surf(X,Y, reshape(noise,sqrt(numel(noise)),sqrt(numel(noise))))
    title('Noise in Provided Function');
    sgtitle('Residual Distribution')
    
    figure(k+7)
    sgtitle('RBF Placement')
    
    for i=1:k
        figure(k+5);
        subplot(2,ceil(total_subplots/2),i+2)
        approx = Iterated_AMLS_construct(data.x_full,xc{i},eps{i},h{i},mult{i});
        approx_grid=reshape(approx,sqrt(numel(approx)),sqrt(numel(approx)));
        surf(X,Y,approx_grid)
        title({strcat('Method'," ",num2str(i),':'),char(options_use(i).label)},'Interpreter', 'none','FontSize',10)
        
        figure(k+6)
        subplot(2,ceil(Res_total_subplots/2),i+1)
        res=approx-data.f_true;
        surf(X,Y,reshape(res,sqrt(numel(res)),sqrt(numel(res))))
        title({strcat('Method'," ",num2str(i),':'),char(options_use(i).label)},'Interpreter', 'none','FontSize',10)
        
        figure(k+7)
        subplot(2,ceil(k/2),i)
        hist3(xc{i},'Nbins',size(X),'CDataMode','auto')
        colorbar
        view(2)
        title({strcat('Method'," ",num2str(i),':'),char(options_use(i).label)},'Interpreter', 'none','FontSize',10)
    end
    
end

%% PLOTTING SECTION
if repeat_plots==1
    %Define strings needed for markers and legends
    full_markers_color={'r';'g';'b';'m';'c';'k'};
    markers_color_use=full_markers_color(methods);
    full_legends=full_options.opt_funct;
    for i=1:numel(options_use)
        legends_use(i)={string(i)+" "+string(options_use(i).label)};
    end
    legends_use=string(legends_use);
    train_text=repmat({' Train'},1,length(legends_use));
    test_text=repmat({' Test'},1,length(legends_use));
    true_text=repmat({' True'},1,length(legends_use));
    error_bars=1;
    
    %Plotting computation time
    figure(k+1)
    clf
    set(gcf, 'Units', 'Normalized', 'OuterPosition', [0 0 1 1]);
    hold on
    if error_bars==0 %Plot Without error bars
        for i=1:size(mean_time,3)
            plot(h_values,mean_time(:,:,i),[char(markers_color_use(i)) '-o'])
        end
    else %Plot with error bars of standard deviation
        for i=1:size(mean_time,3)
            errorbar(h_values,mean_time(:,:,i),time_std(:,:,i),[char(markers_color_use(i)) '-o'])
        end
    end
    xlabel('h Value')
    xticks(h_values)
    ylabel('Mean time to place all RBFs (sec)')
    title(strcat('Computation time comparison, # Repeats=',string(numRepeats)))
    legend((legends_use),'Location','best','Interpreter','none')
    grid on
    grid minor
    set(gca,'YScale','log')
    hold off
    if save_figures==1
        print(strcat('time_comparison_',runLabel),'-dpng')
    end
    
    %plotting accuracy
    figure(k+2)
    clf
    set(gcf, 'Units', 'Normalized', 'OuterPosition', [0 0 1 1]);
    hold on
    
    %Plotting Training Error
    if error_bars==0 %Plot Without error bars
        for i=1:size(mean_err_train,3)
            plot(h_values,mean_err_train(:,:,i),[char(markers_color_use(i)) '-o'])
        end
    else %Plot with error bars of standard deviation
        for i=1:size(mean_err_train,3)
            errorbar(h_values,mean_err_train(:,:,i),err_train_std(:,:,i),[char(markers_color_use(i)) '-o'])
        end
    end
    legend_text=strcat([legends_use], [train_text]);
    
    %Plotting Testing Error
    if test_data==1
        if error_bars==0 %Plot w/o error bars
            for i=1:size(mean_err_train,3)
                plot(h_values,mean_err_test(:,:,i),[char(markers_color_use(i)) '--*'])
            end
        else %Plot with error bars of standard deviation
            for i=1:size(mean_err_test,3)
                errorbar(h_values,mean_err_test(:,:,i),err_test_std(:,:,i),[char(markers_color_use(i)) '--*'])
            end
        end
        legend_text=[legend_text,strcat(legends_use,test_text)];
    end
    
    %Plotting True Error
    if true_error==1
        if error_bars==0 %Plot w/o error bars
            for i=1:size(mean_err_true,3)
                plot(h_values,mean_err_true(:,:,i),[char(markers_color_use(i)) '-.s'])
            end
        else %Plot with error bars of standard deviation
            for i=1:size(mean_err_true,3)
                errorbar(h_values,mean_err_true(:,:,i),err_true_std(:,:,i),[char(markers_color_use(i)) '-.s'])
            end
        end
        legend_text=[legend_text,strcat(legends_use,true_text)];
    end
    
    legend((legend_text),'Location','best','Interpreter', 'none')
    xlabel('h Value')
    xticks(h_values)
    ylabel('Err ((mean(R^2))^.^5)')
    title(strcat('Accuracy Comparison, # Repeats=',string(numRepeats)))
    grid on
    grid minor
    %     set(gca,'YScale','log')
    hold off
    if save_figures==1
        print(strcat('accuracy_comparison_',runLabel),'-dpng')
    end
    
    %Plotting number of RBFs used
    figure(k+3)
    clf
    set(gcf, 'Units', 'Normalized', 'OuterPosition', [0 0 1 1]);
    hold on
    if error_bars==0 %Plot Without error bars
        for i=1:size(mean_used_RBF,3)
            plot(h_values,mean_used_RBF(:,:,i),[char(markers_color_use(i)) '-o'])
        end
    else %Plot with error bars of standard deviation
        for i=1:size(mean_used_RBF,3)
            errorbar(h_values,mean_used_RBF(:,:,i),used_RBF_std(:,:,i),[char(markers_color_use(i)) '-o'])
        end
    end
    xlabel('h Values')
    xticks(h_values)
    ylabel('Mean RBFs used')
    title(strcat('RBFs used comparison, # Repeats=',string(numRepeats)))
    legend((legends_use),'Location','best','Interpreter', 'none')
    grid on
    grid minor
    hold off
    
    if save_figures==1
        print(strcat('RBF_comparison_',runLabel),'-dpng')
    end
    %%
    %Plotting Error History
    if numel(h_values)==1
        
        figure(k+4)
        clf
        set(gcf, 'Units', 'Normalized', 'OuterPosition', [0 0 1 1]);
        hold on
        
        h_plot=1;
        %Plotting Training Error
        if error_bars==0 %Plot Without error bars
            for i=1:size(mean_err_hist_train,4)
                plot(0:size(mean_err_hist_train,1)-1,mean_err_hist_train(:,:,h_plot,i),[char(markers_color_use(i)) '-o'])
            end
        else %Plot with error bars of standard deviation
            for i=1:size(mean_err_hist_train,4)
                errorbar(0:size(mean_err_hist_train,1)-1,mean_err_hist_train(:,:,h_plot,i),err_hist_train_std(:,:,h_plot,i),[char(markers_color_use(i)) '-o'])
            end
        end
        legend_text=strcat([legends_use], [train_text]);
        
        %Plotting Testing Error
        if test_data==1
            if error_bars==0 %Plot w/o error bars
                for i=1:size(mean_err_hist_train,4)
                    plot(0:size(mean_err_hist_test,1)-1,mean_err_hist_test(:,:,h_plot,i),[char(markers_color_use(i)) '--*'])
                end
            else %Plot with error bars of standard deviation
                for i=1:size(mean_err_hist_train,4)
                    errorbar(0:size(mean_err_hist_test,1)-1,mean_err_hist_test(:,:,h_plot,i),err_hist_test_std(:,:,h_plot,i),[char(markers_color_use(i)) '--*'])
                end
            end
            legend_text=[legend_text,strcat(legends_use,test_text)];
        end
        
        %Plotting True Error
        if true_error==1
            if error_bars==0 %Plot w/o error bars
                for i=1:size(mean_err_hist_train,4)
                    plot(0:size(mean_err_hist_true,1)-1,mean_err_hist_true(:,:,h_plot,i),[char(markers_color_use(i)) '-.s'])
                end
            else %Plot with error bars of standard deviation
                for i=1:size(mean_err_hist_train,4)
                    errorbar(0:size(mean_err_hist_true,1)-1,mean_err_hist_true(:,:,h_plot,i),err_hist_true_std(:,:,h_plot,i),[char(markers_color_use(i)) '-.s'])
                end
            end
            legend_text=[legend_text,strcat(legends_use,true_text)];
        end
        
        legend((legend_text),'Location','best','Interpreter', 'none')
        xlabel('Iteration')
        ylabel('Err ((mean(R^2))^.^5)')
        title(strcat('Learning History Comparison, # Repeats=',string(numRepeats)))
        grid on
        grid minor
        set(gca,'YScale','log')
        hold off
        if save_figures==1
            fullfile(output_location,filename)
            print(strcat('LearningHistory_comparison_',runLabel),'-dpng')
        end
        
    else
        
        figure(k+4)
        clf
        set(gcf, 'Units', 'Normalized', 'OuterPosition', [0 0 1 1]);
        hold on
        train_text=repmat({' Train'},1,length(h_values));
        test_text=repmat({' Test'},1,length(h_values));
        true_text=repmat({' True'},1,length(h_values));
        
        methodPlot=1;
        %         %Plotting Training Error
        %         if error_bars==0 %Plot Without error bars
        %             for i=1:size(mean_err_hist_train,3)
        %                 plot(0:size(mean_err_hist_train,1)-1,mean_err_hist_train(:,:,i,methodPlot),'-o')
        %             end
        %         else %Plot with error bars of standard deviation
        %             for i=1:size(mean_err_hist_train,3)
        %                 errorbar(0:size(mean_err_hist_train,1)-1,mean_err_hist_train(:,:,i,methodPlot),err_hist_train_std(:,:,i,methodPlot),'-o')
        %             end
        %         end
        %         legend_text=("h="+num2str(h_values')+test_text');
        
        %         %Plotting Testing Error
        %         if test_data==1
        %             if error_bars==0 %Plot w/o error bars
        %                 for i=1:size(mean_err_hist_train,3)
        %                     plot(0:size(mean_err_hist_test,1)-1,mean_err_hist_test(:,:,i,methodPlot),'--*')
        %                 end
        %             else %Plot with error bars of standard deviation
        %                 for i=1:size(mean_err_hist_train,3)
        %                     errorbar(0:size(mean_err_hist_test,1)-1,mean_err_hist_test(:,:,i,methodPlot),err_hist_test_std(:,:,i,methodPlot),'--*')
        %                 end
        %             end
        %             legend_text=[("h="+num2str(h_values')+test_text')];
        %         end
        
        %Plotting True Error
        if true_error==1
            if error_bars==0 %Plot w/o error bars
                for i=1:size(mean_err_hist_train,3)
                    plot(0:size(mean_err_hist_true,1)-1,mean_err_hist_true(:,:,i,methodPlot),'-.s')
                end
            else %Plot with error bars of standard deviation
                for i=1:size(mean_err_hist_train,3)
                    errorbar(0:size(mean_err_hist_true,1)-1,mean_err_hist_true(:,:,i,methodPlot),err_hist_true_std(:,:,i,methodPlot),'-.s')
                end
            end
            legend_text=[("h="+num2str(h_values')+true_text')];
        end
        
        legend((legend_text),'Location','best','Interpreter', 'none')
        xlabel('Iteration')
        ylabel('Err ((mean(R^2))^.^5)')
        title(strcat('Learning History Comparison, # Repeats=',string(numRepeats)))
        grid on
        grid minor
        set(gca,'YScale','log')
        hold off
        if save_figures==1
            print(strcat('LearningHistory_comparison_',runLabel),'-dpng')
        end
    end
    
end